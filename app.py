# app.py
import os
import re
from dotenv import load_dotenv
import chromadb
import dashscope
from dashscope import Generation, TextEmbedding
from dashscope import Generation

# ============ åŠ è½½ç¯å¢ƒå˜é‡ ============
load_dotenv()
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
# ====================================================

dashscope.base_http_api_url = 'https://dashscope.aliyuncs.com/api/v1' # ä¸ºAPIè¯·æ±‚æä¾›åœ°å€

# ========== é…ç½®é¡¹ (è¯·ä¸build_vector_db.pyä¿æŒä¸€è‡´) ==========
KNOWLEDGE_BASE_DIR = "knowledge_base"
CHROMA_DB_DIR = "db/chroma_db"
COLLECTION_NAME = "linux_commands"


# ====================================================
# è§£ææé—®
def get_embeddings(texts):
    """
    ä½¿ç”¨é˜¿é‡Œäº‘ text-embedding-v3 æ¨¡å‹æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥å‘é‡ã€‚
    """
    if not isinstance(texts, list):
        texts = [texts]
        
    response = TextEmbedding.call(
        model='text-embedding-v3',
        input=texts,
        api_key=DASHSCOPE_API_KEY
    )
    
    if response.status_code == 200:
        # 'dense' æ˜¯é»˜è®¤çš„å‘é‡ç±»å‹
        embeddings = [item['embedding'] for item in response.output['embeddings']]
        return embeddings
    else:
        raise Exception(f"åµŒå…¥APIè°ƒç”¨å¤±è´¥: {response.code}, {response.message}")

# æ£€ç´¢æ•°æ®åº“
def retrieve_context(query, n_results=3):
    """
    æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡æ–‡æœ¬å—ã€‚
    ä½¿ç”¨é˜¿é‡Œäº‘ text-embedding-v3 è¿›è¡ŒæŸ¥è¯¢ã€‚
    """
    try:
        # 1. ä¸ºæŸ¥è¯¢æ–‡æœ¬ç”ŸæˆåµŒå…¥
        query_embedding = get_embeddings(query)[0] # get_embeddings è¿”å›åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
        
        # 2. è¿æ¥åˆ°æ•°æ®åº“å¹¶æŸ¥è¯¢
        client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
        collection = client.get_collection(name=COLLECTION_NAME)
        
        results = collection.query(
            query_embeddings=[query_embedding], # æ³¨æ„è¿™é‡Œæ˜¯ query_embeddings
            n_results=n_results
        )
        return results['documents'][0] if results['documents'] else []
        
    except Exception as e:
        print(f"æ£€ç´¢æ—¶å‡ºé”™: {e}")
        return []

# è·å–æ¨¡å‹å›ç­”
def call_qwen_api(prompt, model='qwen3-max'):
    """
    è°ƒç”¨é€šä¹‰åƒé—®APIç”Ÿæˆå›ç­”ã€‚
    """
    # è®¾ç½®API Key
    Generation.api_key = DASHSCOPE_API_KEY

    try:
        response = Generation.call(
            api_key=DASHSCOPE_API_KEY,
            model=model,
            prompt=prompt,
            max_tokens=1024,
            temperature=0.5,  # é™ä½éšæœºæ€§ï¼Œè®©å›ç­”æ›´ç¨³å®š
        )

        if response.status_code == 200:
            return response.output.choices[0].message.content
        else:
            return f"âŒ APIè°ƒç”¨å¤±è´¥: {response.code}, {response.message}"

    except Exception as e:
        return f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"

# è·å–åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢çš„æé—®è¯
def get_retrieve_prompt(user_question):
    return f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„Linuxå‘½ä»¤çŸ¥è¯†åº“è®¿é—®æ§åˆ¶å™¨ã€‚
è¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œä¸¥æ ¼åˆ¤æ–­æ˜¯å¦éœ€è¦ä»Linuxå‘½ä»¤çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¿¡æ¯ã€‚
åªå…è®¸åœ¨ä»¥ä¸‹æƒ…å†µå›ç­”"éœ€è¦"ï¼š
1. ç”¨æˆ·æ˜ç¡®è¯¢é—®æŸä¸ªLinuxå‘½ä»¤çš„å…·ä½“ç”¨æ³•ã€å‚æ•°ã€ä½œç”¨
2. ç”¨æˆ·é—®é¢˜ä¸­åŒ…å«"å‘½ä»¤"ã€"æŒ‡ä»¤"ã€"ç”¨æ³•"ã€"è¯­æ³•"ç­‰å…³é”®è¯
3. ç”¨æˆ·ç›´æ¥æåŠå…·ä½“çš„Linuxå‘½ä»¤åç§°ï¼ˆå¦‚su, ls, grepç­‰ï¼‰

åœ¨ä»¥ä¸‹æƒ…å†µå¿…é¡»å›ç­”"ä¸éœ€è¦"ï¼š
1. ç”¨æˆ·åœ¨è®¾ç½®æ˜µç§°ã€é—®å€™ã€å‘Šåˆ«ç­‰ç¤¾äº¤äº’åŠ¨
2. ç”¨æˆ·é—®é¢˜ä¸Linuxå‘½ä»¤æ— å…³
3. ç”¨æˆ·åªæ˜¯æ³›æ³›è€Œè°ˆ"Linux"ä½†æœªæ¶‰åŠå…·ä½“å‘½ä»¤
    
ç”¨æˆ·é—®é¢˜{user_question}
    
è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚å›ç­”ï¼Œåªèƒ½å›ç­”"éœ€è¦"æˆ–"ä¸éœ€è¦"ï¼Œä¸èƒ½æ·»åŠ å…¶ä»–å†…å®¹ã€‚
"""

# åˆ¤æ–­æ˜¯å¦éœ€è¦ä»æ•°æ®åº“ä¸­æ£€ç´¢ä¿¡æ¯
def is_need_retrieve(user_question):
    """
    é€šè¿‡å¤šé‡åˆ¤æ–­æ¥å†³å®šæ˜¯å¦éœ€è¦å»æ£€ç´¢æ•°æ®åº“
    åˆ¤æ–­ä¸€ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«Linuxå‘½ä»¤å…³é”®è¯
    åˆ¤æ–­äºŒï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«"å‘½ä»¤"ã€"æŒ‡ä»¤"ã€"ç”¨æ³•"ã€"è¯­æ³•"ã€"å‚æ•°"ç­‰å…³é”®è¯
    åˆ¤æ–­ä¸‰ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¤¾äº¤æˆ–è®¾ç½®è¯·æ±‚
    """

    # è®¾ç½®ç¡¬æ€§è§„åˆ™è¿‡æ»¤
    user_question = user_question.strip().lower()

    result = None # é»˜è®¤ä¸ºç©º

    # åˆ¤æ–­ä¸€ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«Linuxå‘½ä»¤å…³é”®è¯
    linux_commands = [
    "man", "help", "info", "shutdown", "reboot", "halt", "poweroff", "pwd", 
    "cd", "tree", "mkdir", "touch", "ls", "cp", "mv", "rm", "rmdir", "ln", 
    "readlink", "find", "xargs", "rename", "basename", "dirname", "chattr",
    "lsattr", "file", "md5sum", "chown", "chmod", "chgrp", "umask", "cat",
    "tac", "more", "less", "head", "tail", "tailf", "cut", "split", "paste",
    "sort", "join", "uniq", "wc", "iconv", "dos2unix", "diff", "vimdiff", 
    "rev", "tr", "od", "tee", "vi", "vim", "grep", "sed", "awk", "uname", 
    "hostname", "demsg", "stat", "du", "date", "echo", "watch", "which", 
    "whereis", "locate", "updatedb", "tar", "gzip", "zip", "unzip", "scp", 
    "rsync", "useradd", "usermod", "userdel", "groupadd", "groupdel", "passwd", 
    "chage", "chpasswd", "su", "visudo", "sudo", "id", "w", "who", "users", 
    "whoami", "last", "lastb", "latslog", "fdisk", "partprobe", "tune2fs", 
    "parted", "mkfs", "dumpe2fs", "resize2fs", "fsck", "dd", "mount", 
    "umount", "df", "mkswap", "swapon", "swapoff", "sync", "ps", 
    "pstree", "pgrep", "kill", "killall", "pkill", "top", "nice", 
    "renice", "nohup", "strace", "ltrace", "runlevel", "init", "service",
    "ifconfig", "ifup", "ifdown", "route", "arp", "ip", "netstat", "ss", 
    "ping", "traceroute", "arping", "telnet", "curl", "nc", "ssh", "wget", 
    "mail", "mailq", "nslookup", "dig", "host", "nmap", "tcpdump", "lsof", 
    "uptime", "free", "iftop", "vmstat", "mpstat", "iostat", "iotop", "sar", 
    "chkconfig", "ntsysv", "setup", "ethtool", "mii-tool", "dmidecode", 
    "lspci", "ipcs", "ipcrm", "rpm", "yum", ":", "source", "test", "alias", 
    "unalias", "bg", "fg", "jobs", "break", "continue", "eval", "exit", 
    "logout", "export", "history", "read", "type", "ulimit", "unset"]

    words = re.findall(r'\b[a-zA-Z]+\b', user_question) # ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰å•è¯
    if any(cmd in words for cmd in linux_commands):
        return true

    # åˆ¤æ–­äºŒï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«"å‘½ä»¤"ã€"æŒ‡ä»¤"ã€"ç”¨æ³•"ã€"è¯­æ³•"ç­‰å…³é”®è¯
    keywords = ["å‘½ä»¤", "æŒ‡ä»¤", "ç”¨æ³•", "è¯­æ³•", "å‚æ•°"]
    if any(keyword in user_question for keyword in keywords):
        return True
    
    # åˆ¤æ–­ä¸‰ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¤¾äº¤æˆ–è®¾ç½®è¯·æ±‚
    social_keywords = ["ä½ å¥½", "ç§°å‘¼", "åå­—", "hi", "hello", "æ—©ä¸Šå¥½", "æ™šä¸Šå¥½", "å†è§", "bye"]
    if any(keyword in user_question for keyword in social_keywords):
        return False
    
    # æ— æ³•åˆ¤æ–­åˆ™è¿”å›Noneï¼Œäº¤ç»™å¤§æ¨¡å‹åˆ¤æ–­
    return None

# è·å–ç­”æ¡ˆ
def get_answer(user_question, history_tuples=None):
    """
    ä¸»å‡½æ•°ï¼šç»“åˆRAGå’ŒLLMï¼Œç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
    æ”¯æŒï¼š
    - ä»çŸ¥è¯†åº“æ£€ç´¢ï¼ˆRAGï¼‰
    - ä½¿ç”¨å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆè®°å¿†ï¼‰
    - æ··åˆæ¨¡å¼ï¼šæœ‰çŸ¥è¯†åº“å°±ç”¨çŸ¥è¯†åº“ï¼Œæ²¡æœ‰å°±ç”¨é€šç”¨èƒ½åŠ›
    """
    if history_tuples is None:
        history_tuples = []

    # ============ 1. æ„é€ å¯¹è¯å†å²ä¸Šä¸‹æ–‡ ============
    history_context = ""
    if history_tuples:
        # å°†å†å²å¯¹è¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œä½œä¸ºä¸Šä¸‹æ–‡
        history_lines = []
        for user_msg, ai_msg in history_tuples:
            history_lines.append(f"ç”¨æˆ·ï¼š{user_msg}")
            history_lines.append(f"AIï¼š{ai_msg}")
        history_context = "\n\n".join(history_lines)
        history_context = f"ã€è¿‡å¾€å¯¹è¯ã€‘\n{history_context}\n\n"

    # ============ 2. åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢æ•°æ®åº“ ============

    # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ¤æ–­
    judge_response = is_need_retrieve(user_question)

    # åœ¨éœ€è¦æ—¶æ£€ç´¢æ•°æ®åº“
    if judge_response == True:
        contexts = retrieve_context(user_question, n_results=2)
    elif judge_response == False:
        contexts = []
    else:
        #è·å–æç¤ºè¯
        judge_prompt = get_retrieve_prompt(user_question)
        judge_response2 = call_qwen_api(judge_prompt)
        if "éœ€è¦" in judge_response2:
            contexts = retrieve_context(user_question, n_results=2)
        else:
            contexts = []

    # ============ 3. æ„é€ æœ€ç»ˆPrompt ============
    if contexts:
        # æœ‰çŸ¥è¯†åº“ä¿¡æ¯ï¼šä½¿ç”¨RAGæ¨¡å¼ï¼ˆé«˜å‡†ç¡®ï¼‰
        context_str = "\n\n".join(contexts)
        final_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Linuxå‘½ä»¤å­¦ä¹ åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºç”¨æˆ·å­¦ä¹ Linuxå‘½ä»¤æä¾›å¸®åŠ©ã€‚ä»¥ä¸‹æ˜¯æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š

ã€å‚è€ƒä¿¡æ¯ã€‘
{context_str}

{history_context}  # â† æ³¨å…¥å†å²ä¸Šä¸‹æ–‡ï¼

è¯·æ ¹æ®ä»¥ä¸Šæä¾›çš„çŸ¥è¯†åº“å†…å®¹ï¼Œä¸ºç”¨æˆ·è§£ç­”é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ä¸¥æ ¼åŸºäºçŸ¥è¯†åº“ä¸­çš„ä¿¡æ¯å›ç­”ï¼Œä¸è¦æ·»åŠ çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„å†…å®¹
2. å›ç­”è¦æ¸…æ™°ã€ç®€æ´ã€ä¸“ä¸šï¼Œé€‚åˆLinuxå­¦ä¹ è€…ç†è§£
3. å¦‚æœçŸ¥è¯†åº“ä¸­åŒ…å«å‘½ä»¤ç¤ºä¾‹ï¼Œè¯·å®Œæ•´å±•ç¤ºå¹¶è§£é‡Šæ¯ä¸ªå‚æ•°çš„ä½œç”¨
4. å¦‚æœçŸ¥è¯†åº“ä¸­åŒ…å«å¤šä¸ªç›¸å…³ä¿¡æ¯ï¼Œè¯·æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆå¦‚ï¼šæ ¹æ®Linuxå‘½ä»¤æ‰‹å†Œ/æ ¹æ®çŸ¥è¯†åº“æ–‡æ¡£ï¼‰
6. å¦‚æœç”¨æˆ·è¯¢é—®çš„å‘½ä»¤æœ‰å®‰å…¨é£é™©æˆ–éœ€è¦æ³¨æ„äº‹é¡¹ï¼Œè¯·ç‰¹åˆ«æé†’
7. ä¿æŒæ•™è‚²æ€§å’Œå®ç”¨æ€§ï¼Œå¸®åŠ©ç”¨æˆ·çœŸæ­£ç†è§£å‘½ä»¤çš„ä½¿ç”¨æ–¹æ³•å’Œæ³¨æ„äº‹é¡¹
8. éµå®ˆç”¨æˆ·åœ¨å†å²å¯¹è¯ä¸­æå‡ºçš„è¦æ±‚
ã€å½“å‰é—®é¢˜ã€‘
{user_question}

ç°åœ¨è¯·åŸºäºçŸ¥è¯†åº“å†…å®¹ç»™å‡ºä¸“ä¸šè§£ç­”ï¼š"""
    else:
        if judge_response == None:
            retrieve_prompt = "ç»“æœæ£€ç´¢ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å…·ä½“ä¿¡æ¯ã€‚"
        else:
            retrieve_prompt = "ç”¨æˆ·çš„é—®é¢˜ä¼¼ä¹ä¸Linuxå‘½ä»¤æ— å…³ï¼Œå°†åŸºäºé€šç”¨çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
        # æ²¡æœ‰çŸ¥è¯†åº“ä¿¡æ¯ï¼šä½¿ç”¨é€šç”¨é—®ç­”æ¨¡å¼
        final_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Linuxå‘½ä»¤å­¦ä¹ åŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºç”¨æˆ·å­¦ä¹ Linuxå‘½ä»¤æä¾›å¸®åŠ©ã€‚{retrieve_prompt}
{history_context}  # â† æ³¨å…¥å†å²ä¸Šä¸‹æ–‡ï¼

<system_info>
- å½“å‰æ˜¯é€šç”¨çŸ¥è¯†å›ç­”æ¨¡å¼
- è¯·åŸºäºä½ çš„è®­ç»ƒçŸ¥è¯†å›ç­”Linuxç›¸å…³é—®é¢˜
- é‡ç‚¹å›ç­”æŠ€æœ¯å‡†ç¡®æ€§ï¼Œé¿å…çŒœæµ‹ä¸ç¡®å®šçš„ä¿¡æ¯
- å¦‚æœé—®é¢˜è¶…å‡ºLinuxèŒƒå›´ï¼Œè¯·å‹å¥½å¼•å¯¼å›Linuxä¸»é¢˜
- ä¿æŒä¸“ä¸šã€æ•™è‚²æ€§çš„è¯­æ°”
</system_info>

è¯·æ ¹æ®ä½ çš„é€šç”¨çŸ¥è¯†ä¸ºç”¨æˆ·è§£ç­”é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ä»…å›ç­”ä¸Linuxå‘½ä»¤ã€ç³»ç»Ÿç®¡ç†ã€Shellè„šæœ¬ç›¸å…³çš„æŠ€æœ¯é—®é¢˜
2. ç¡®ä¿æŠ€æœ¯ç»†èŠ‚çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å‘½ä»¤è¯­æ³•ã€å‚æ•°å’Œä½¿ç”¨åœºæ™¯
3. æä¾›å®ç”¨çš„ç¤ºä¾‹ä»£ç ï¼Œå¹¶è§£é‡Šå…³é”®éƒ¨åˆ†
4. æ˜ç¡®æ ‡æ³¨è¿™æ˜¯åŸºäºé€šç”¨çŸ¥è¯†çš„å›ç­”ï¼Œè€Œéæ¥è‡ªç‰¹å®šçŸ¥è¯†åº“
5. å¦‚æœå¯¹æŸäº›ç»†èŠ‚ä¸ç¡®å®šï¼Œè¯·è¯´æ˜å¹¶å»ºè®®ç”¨æˆ·æŸ¥é˜…å®˜æ–¹æ–‡æ¡£
6. å¯¹äºå¤æ‚å‘½ä»¤ï¼Œåˆ†æ­¥éª¤è§£é‡Šä½¿ç”¨æ–¹æ³•
7. æé†’ç”¨æˆ·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨å‘½ä»¤å‰è¦å……åˆ†æµ‹è¯•

ã€å½“å‰é—®é¢˜ã€‘
{user_question}

ç°åœ¨è¯·åŸºäºä½ çš„é€šç”¨LinuxçŸ¥è¯†ç»™å‡ºä¸“ä¸šè§£ç­”ï¼Œå¹¶åœ¨å¼€å¤´è¯´æ˜"{retrieve_prompt}ä»¥ä¸‹åŸºäºé€šç”¨LinuxçŸ¥è¯†è§£ç­”ï¼š"ï¼š"""

    # ============ 4. è°ƒç”¨å¤§æ¨¡å‹ ============
    answer = call_qwen_api(final_prompt)
    return answer


# =================== è¿è¡Œæµ‹è¯• ===================
try:
    import gradio as gr

    def chat_interface(user_input, history_messages=None):
        """
        Gradioç•Œé¢çš„å¤„ç†å‡½æ•°ï¼Œä½¿ç”¨openai-style messages æ ¼å¼
        """
        if history_messages is None:
            history_messages = []

        if not user_input.strip():
            return "", history_messages, history_messages
        
        # å°† history_messages ä»å­—å…¸æ ¼å¼è½¬æ¢ä¸ºå…ƒç»„æ ¼å¼
        history_tuples = []
        for i in range(0, len(history_messages), 2):
            if i + 1 < len(history_messages):
                user_msg = history_messages[i]["content"]
                ai_msg = history_messages[i+1]["content"]
                history_tuples.append((user_msg, ai_msg))
        
        # è·å–AIå›ç­” (ç¡®ä¿ get_answer æ”¯æŒ messages æ ¼å¼çš„å†å²)
        ai_response = get_answer(user_input, history_tuples)
        
        # å°†ç”¨æˆ·è¾“å…¥å’ŒAIå›ç­”æ·»åŠ åˆ°å†å²
        history_messages.append({"role": "user", "content": user_input})
        history_messages.append({"role": "assistant", "content": ai_response})
        
        # è¿”å›å€¼ï¼šæ¸…ç©ºè¾“å…¥æ¡†ï¼Œæ›´æ–°èŠå¤©å†å²ï¼Œæ›´æ–°çŠ¶æ€
        return "", history_messages, history_messages

    # åˆ›å»ºGradioç•Œé¢
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ§ Linuxå‘½ä»¤å°åŠ©æ‰‹")
        gr.Markdown("è¾“å…¥å…³äºLinuxå‘½ä»¤çš„é—®é¢˜ï¼Œæˆ‘ä¼šæ ¹æ®å®˜æ–¹æ–‡æ¡£ä¸ºä½ è§£ç­”ã€‚")
        
        # ä½¿ç”¨Chatbotç»„ä»¶æ˜¾ç¤ºå†å²å¯¹è¯
        # æ³¨æ„ï¼šChatbotæœŸæœ›çš„æ ¼å¼æ˜¯ [[msg1, msg2], ...]ï¼Œå…¶ä¸­msg1æ˜¯ç”¨æˆ·ï¼Œmsg2æ˜¯AI
        chatbot = gr.Chatbot(label="å¯¹è¯å†å²", type="messages", height=650)
        
        with gr.Row():
            with gr.Column(scale=4):
                user_input = gr.Textbox(placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•æŸ¥æ‰¾ä¸€ä¸ªæ–‡ä»¶ï¼Ÿ", label="ä½ çš„é—®é¢˜", scale=4)
            with gr.Column(scale=1):
                submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
        
        # Stateç»„ä»¶ç°åœ¨å­˜å‚¨å…ƒç»„æ ¼å¼çš„å†å²
        history_state = gr.State([]) 
        
        # è®¾ç½®æŒ‰é’®ç‚¹å‡»äº‹ä»¶ï¼Œoutputsé¡ºåºå¿…é¡»æ˜¯ [user_input, chatbot, history_state]
        submit_btn.click(
            fn=chat_interface, 
            inputs=[user_input, history_state], 
            outputs=[user_input, chatbot, history_state] # è¿™é‡Œå®šä¹‰äº†è¾“å‡ºé¡ºåº
        )
        # ä¹Ÿæ”¯æŒæŒ‰å›è½¦é”®æäº¤
        user_input.submit(
            fn=chat_interface, 
            inputs=[user_input, history_state], 
            outputs=[user_input, chatbot, history_state]
        )

    # å¯åŠ¨åº”ç”¨ï¼ˆä»…å½“ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶ï¼‰
    if __name__ == "__main__":
        print("\nğŸš€ å¯åŠ¨Webç•Œé¢...")
        demo.launch(server_name="127.0.0.1", server_port=7860, share=False)

except ImportError:

    print("\n Gradioæœªå®‰è£…ã€‚å¦‚éœ€Webç•Œé¢ï¼Œè¯·è¿è¡Œ: pip install gradio")
